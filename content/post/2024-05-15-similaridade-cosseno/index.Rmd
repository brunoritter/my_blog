---
title: "A similaridade de cosseno, aproximações e eficência de código"
author: "Bruno Ritter"
date: 2024-05-14
categories: ["Data Science"]
tags: ["geometria", "data science", "feature engineering"]
output:
  blogdown::html_page:
    keep_md: true
---

```{r setup, include=FALSE}
library(reticulate)
library(knitr)
use_condaenv("C:/Users/ritte/miniconda3/envs/my_blog")

knitr::opts_chunk$set(echo = TRUE, results = 'asis', message = FALSE, warning = FALSE)
```
# Pra quem tem pressa, resumão:

Neste post, exploramos a aplicação da similaridade de cosseno para calcular distâncias entre pontos geográficos, utilizando uma aproximação esférica da Terra. Embora tenhamos simplificado a forma da Terra para um esferoide perfeito, os resultados demonstraram que essa abordagem introduz erros mínimos, tornando-a uma alternativa viável e eficiente para cálculos de distâncias em grande escala.

Ao compararmos o método da similaridade de cosseno com a solução mais precisa do *geopy*, observamos que a diferença nas distâncias calculadas foi insignificante para a maioria das aplicações práticas, com um erro médio de apenas 0.2%. Em troca, temos um código rodando centenas de vezes mais rápido, o que se torna crucial quando lidamos com grandes volumes de dados.

Esse exercício, além de demonstrar como a distância de cosseno é idealizada e calculada, ilustra como um bom entendimento do problema pode levar a soluções altamente eficientes e suficientemente precisas para problemas complexos em ciência de dados. 

Espero que as técnicas e conceitos discutidos aqui possam ser úteis em suas próprias explorações e desafios de data science :)

# O desafio

Depois de um tempo completamente dedicado à estudar matemática como os antigos faziam (lapiseira e papel), e usando python majoritariamente para fins astrofísicos, resolvi procurar alguns desafios relacionados a dados para exercitar um pouco o ferramental mais utilizado na prática da ciência de dados, como pandas, matplotlib e scikit-learn. Pesquisando no [r/datascience](https://www.reddit.com/r/datascience/), vi um colega indicando os desafios do [data-puzzles](https://data-puzzles.com/).

Um dos primeiros desafios é o [travel planer](https://data-puzzles.com/challenges/travel-planner/), com a tag ‘feature engineering’. Considero que essa é uma das habilidades mais relevantes para um cientista de dados. A máxima “garbage in, garbage out” é repetida exaustivamente por uma razão: modelos simples com as features corretas podem operar milagres. O desafio te entrega uma base de dados com as coordenadas geográficas (latitude e longitude) de todas as capitais nacionais do mundo, e um objetivo simples: encontrar as duas capitais com a maior distância possível entre elas, considerando uma trajetória na superfície terrestre.

Não é difícil relacionar o exercício à problemas práticos. Trabalhei em um projeto de varejo fazendo previsões de venda e recomendação de mix de produtos para lojas ao longo de todo o território nacional. Um dos experimentos bem sucedidos que fizemos foi incluir informações sobre o entorno geográfico de cada ponto de venda nos modelos. Para dar alguns exemplos, pensando em um raio de 5Km a partir da loja, podemos considerar: 

* Existem escolas? Quantas?   
* Existem outros estabelecimentos? De quais tipos?
* Qual a distribuição de idade dos moradores?

Para construir um dataset com este nível de informação, é necessário resolver o mesmo problema proposto pelo puzzle: encontrar distâncias na superfície terrestre a partir de coordenadas geográficas.

Vamos explorar como isso pode ser feito. O código abaixo carrega a base de dados e exibe as 5 primeiras linhas.

```{python chunk1, echo=TRUE}
import pandas as pd

capitals_df = pd.read_csv('https://raw.githubusercontent.com/hyperc54/data-puzzles-assets/master/features/travel/worldcapitals_light.csv')
print(capitals_df.shape)
print(capitals_df.head().to_markdown())

```

Para encontrar a melhor estratégia de solução, vamos fazer algumas considerações geométricas. Sabemos que a Terra não é exatamente uma esfera. A rotação do planeta faz com que ele seja achatado nos polos e alongado no equador, formando o que chamamos de um “esferóide oblato”.

<div style="text-align: center;">
  <img src="/images/tipos-de-esferoides.jpg" alt="tipos de esferóides"/>
  <p style="font-size: 0.8em; color: gray; margin-top: 5px;">
    A imagem mostra lado a lado um esferóide oblato (formato aproximado da Terra), um esferóide prolato (no qual o achatamento ocorre no equador, e o alongamento nos polos), e uma esfera. Peguei a imagem <a href="https://andydx.com/descubrimientos/la-verdadera-forma-de-la-tierra/">aqui</a>.
  </p>
</div>


No entanto, seria extremamente vantajoso tratar este problema assumindo que a Terra é perfeitamente esférica. Isso nos permitiria utilizar a métrica de similaridade de cosseno para medir distâncias, simplificando alguns cálculos que seriam necessários para refletir a variação do raio terreste. Sabendo as dimensões dos eixos do esferóide oblato correspondente às dimensões terrestres, podemos calcular o nível de achatamento do esferóide com a seguinte equação:

$$
f\ =\ \frac{a-b}{a}
$$
Onde $\mathbf{a}$ é o raio equatorial (6.378Km), e $\mathbf{b}$ é o raio polar (6.357Km). Substituindo os valores, calculamos $\mathbf{f} = 0.0034$. Esse valor é muito pequeno, e indica que a diferença entre o eixo maior e o menor é de aproximadamente 1 parte em 298.257. Considero seguro seguirmos em frente com a aproximação esférica e o uso da similaridade de cosseno. 

Vamos conferir os detalhes desta métrica.


# A similaridade de cosseno

A similaridade de cosseno é amplamente utilizada em ciência de dados. Algumas das suas aplicações mais importantes incluem: algoritmos de clusterização (como K-means e K-NN), sistemas de recomendação (filtragem colaborativa) e embeddings de palavras e sentenças em modelos de linguagem de grande escala (LLMs).

A aplicação geográfica que estamos fazendo é uma demonstração muito didática da motivação da similaridade de cosseno. A partir do centro do planeta Terra, traçamos uma linha reta até a posição de uma capital qualquer (vamos chamar de capital A). Esta linha é o vetor posição $\mathbf{p}_1$. Fazemos o mesmo procedimento para uma segunda capital (Capital B) para encontrar o vetor $\mathbf{p}_2$. O ângulo $\theta$ entre os vetores $\mathbf{p}_1$ e $\mathbf{p}_2$ é proporcional à distância na superfície terrestre entre as capitais. O cosseno deste ângulo é a similaridade cosseno entre os vetores.


<div style="text-align: center;">
<img src="/images/cos_dist-1.png" alt="similaridade cosseno" width=300/>

<p style="font-size: 0.8em; color: gray; margin-top: 5px;">O ângulo entre duas capitais.</p>
</div>

Enquanto nessa aplicação nossos vetores possuem 3 dimensões que representam as dimensões espaciais, em uma aplicação de clusterização cada dimensão dos vetores representaria uma característica dos objetos sendo agrupados (usários de um app, por exemplo). Em uma aplicação para NLP, o espaço seria composto pelas $\mathbf{n}$ dimensões do word embedding, representando características latentes que capturam significados semânticos das palavras.

O *scipy* já possui uma função para computar o cosseno entre vetores (experimente usar `from scipy.spatial.distance import cosine`). Mas pelo bem da educação, vamos implementar o cálculo "do zero". Matematicamente, podemos partir da definição do produto interno entre dois vetores: 

$$
\cos(\theta) = \frac{\mathbf{p}_1 \cdot \mathbf{p}_2}{\|\mathbf{p}_1\| \|\mathbf{p}_2\|}
$$
Ou seja, para encontrar a similaridade de cosseno entre dois vetores, calculamos o produto interno entre eles, e dividimos pelo produto dos seus módulos.

# Implementação do cálculo da similaridade de cosseno

Podemos usar um pouco de trigonometria para descrever os nossos vetores de posição - originalmente descritos em coordenadas esféricas - em termos das 3 coordenadas espaciais cartesianas \(*x,y* e *z*\). O diagrama abaixo mostra como os senos e cossenos dos ângulos que dão as coordenadas esféricas podem ser utilizados para fazer a transformação.

<div style="text-align: center;">
<img src="/images/polar_to_cartesian.png" alt="coordenadas polares para cartesianas"  width=300/>

<p style="font-size: 0.8em; color: gray; margin-top: 5px;">
    A relação entre coordenadas esféricas e coordenadas cartesianas. Peguei a imagem <a             href="https://www.researchgate.net/figure/Visualization-of-coordinates-in-both-cartesian-and-spherical-systems_fig2_303542946">aqui</a>.
  </p>
</div>

Usamos o numpy para fazer as operações:

```{python chunk2, echo=TRUE}
import numpy as np

# Converte os ângulos para radianos
capitals_df[['lat_radians', 'lng_radians']] = capitals_df[['lat', 'lng']].apply(np.radians)

# Usa as relações demonstradas no diagrama acima para encontrar as coordenadas cartesianas
capitals_df['x'] = np.cos(capitals_df['lat_radians']) * np.cos(capitals_df['lng_radians'])
capitals_df['y'] = np.cos(capitals_df['lat_radians']) * np.sin(capitals_df['lng_radians'])
capitals_df['z'] = np.sin(capitals_df['lat_radians'])
```

Perceba que, ao fazer essas transformações, os vetores são normalizados. Na prática, isso significa que podemos simplificar o cálculo da distância para $\cos(\theta) = \mathbf{p}_1 \cdot \mathbf{p}_2$\. Precisamos calcular o produto interno entre todos os vetores posição das nossas capitais. Felizmente, a àlgebra linear está a nosso favor. Se agregarmos todos os vetores posição em uma matriz, basta multiplicá-la pela sua transposta para obter os produtos internos. O numpy segue sendo a ferramenta adequada:

```{python chunk3, echo=TRUE}
# Cria matriz de vetores de posição
matrix = capitals_df[['x', 'y', 'z']].to_numpy()

# Calcula cosseno entre vetores
cos = np.clip(np.dot(matrix, matrix.T), -1, 1) 
```

Com os ângulos calculados, é simples de encontrar as distâncias, considerando que as trajetórias são os arcos que conectam os vetores, que possuem módulos constantes iguais ao raio da Terra (na nossa aproximação esférica). Vamos considerar que o raio da nossa Terra redonda é o ponto médio entre o raio polar e o raio equatorial.

```{python chunk4, echo=TRUE}
# Calcula raio da Terra redonda
min_earth_r = 6357
max_earth_r = 6378
mid_earth_r = (min_earth_r+max_earth_r)/2

# Computa os tamanhos dos arcos em Km
theta = np.arccos(cos)
pairwise_distances = theta * mid_earth_r
```

Pronto! Tudo calculado. Para encontrar a resposta do quiz (os nomes das duas capitais com maior distância entre si), buscamos os índices da maior distância armazenada nessa matriz.

```{python chunk5, echo=TRUE}
# Encontra e armazena os índíces das duas capitais mais distantes, assim como o módulo da separação entre elas 
max_distance = np.amax(pairwise_distances)
city_indexes = np.unravel_index(np.argmax(theta), theta.shape)
city_a = capitals_df['city'][city_indexes[0]]
city_b = capitals_df['city'][city_indexes[1]]

# Imprime o resultado
print(f"As capitais mais distantes são {city_a} e {city_b}, com uma separação de {np.round(max_distance, 1)} Km")
```

Agora podemos conferir o gabarito e colocar nossa aproximação esférica a prova. Foi ou não uma boa aproximação? O site tem um notebook com uma solução diferente implementada. Vamos ver.

# O nosso método VS o "método correto"

A solução do gabarito usa um método mais preciso para calcular as distâncias, disponível na função *distance* do *geopy*. E, em contraste com a operação matricial que demonstrei, faz um loop duplo por todas as linhas do dataset, extraindo os valores de latitude e longitude e os utilizando como input para a função do *geopy*. 

```{python chunk6, echo=TRUE}
from geopy import distance

geopy_pairwise_distances = np.empty([capitals_df.shape[0], capitals_df.shape[0]])

# Fill in the blanks, fortunately, there's 'only' <300 capitals!
for i in range(capitals_df.shape[0]):
    for j in range(capitals_df.shape[0]):
        lat_i = capitals_df.iloc[i]['lat']
        lng_i = capitals_df.iloc[i]['lng']
        lat_j = capitals_df.iloc[j]['lat']
        lng_j = capitals_df.iloc[j]['lng']

        geopy_pairwise_distances[i][j] = distance.distance(
            (lat_i, lng_i),
            (lat_j, lng_j)
        ).km

# Retrieve maximum value
geopy_max_distance = np.amax(geopy_pairwise_distances)

# Retrive indices of the element(s)
max_elements_indices = np.where(geopy_pairwise_distances == geopy_max_distance)

i_city1_max, i_city2_max = max_elements_indices[0]

city1_max = capitals_df.iloc[i_city1_max].city
city2_max = capitals_df.iloc[i_city2_max].city

print(f"As capitais mais distantes são {city1_max} e {city2_max}, com uma separação de {np.round(geopy_max_distance, 1)} Km")
```

Hm, erramos. 

Mas temos algumas coisas interessantes acontecendo. 

Primeiro, note que Adamstown faz parte do par nos dois métodos, e que as maiores distâncias encontradas por cada método diferem em menos de 10Km. Dada a ordem de grandeza das distâncias medidas, e considerando que a extensão espacial de uma capital costuma ser maior do que 10Km, na prática, podemos dizer que essa diferença é insignificante. 

Assumimos no ínicio, com o argumento do achatamento desprezível, que os erros seriam pequenos. Agora, temos as distâncias calculadas tanto pelo método exato, quanto pela aproximação. 

Vamos calcular os erros para cada par de capitais.


```{python chunk8, echo=TRUE}
# Calcula os erros em Km e em termos de erro relativo (MAPE)
upper_triangle_indices = np.triu_indices_from(pairwise_distances, k=1)
pairwise_distances_flat = pairwise_distances[upper_triangle_indices]
geopy_pairwise_distances_flat = geopy_pairwise_distances[upper_triangle_indices]
error_km = np.abs(pairwise_distances_flat - geopy_pairwise_distances_flat)
error_relative = (error_km/geopy_pairwise_distances_flat)*100

print(f"O método dos cossenos tem um erro médio de {np.round(np.mean(error_km),1)} Km ({np.round(np.mean(error_relative), 2)}%)")
```

E vamos ver como estes erros se distribuem.

```{python chunk9, echo=TRUE}
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 15))
plt.subplot(2, 2, 1)
plt.hist(error_km, bins=40)
plt.xlabel('Error (Km)')
plt.ylabel('Frequency')
plt.title('Error distribution (Km)')

plt.subplot(2, 2, 2)
plt.hist(error_relative, bins=40)
plt.xlabel('Error (%)')
plt.ylabel('Frequency')
plt.title('Error distribution (relative)')

plt.subplot(2, 1, 2)
plt.boxplot(error_relative, vert=False, patch_artist=True, boxprops=dict(facecolor='orange'));
plt.ylabel('Percentual error (%)')
plt.title('Relative error boxplot')

plt.tight_layout()
plt.show()
```

A aproximação esférica resulta em erros muito pequenos, praticamente desprezíveis. Quase 3/4 da distâncias apresentam menos 0.2% de erro. Nenhuma distância apresenta erro maior do que 35Km. Para a maior parte das aplicações, features que dependam da distância entre locais na superfície do globo não devem apresentar diferenças significativas se as distâncias forem calculadas por um ou outro destes dois métodos.

Mas este ainda não é último argumento que pode ser feito a favor do método proposto. Quando rodei o código disponibilizado pelo gabarito notei que o tempo de execução foi gritantemente maior do que o tempo de execução do código que implementei. Rodei tudo de novo, dessa vez usando a biblioteca *time* para monitorar o tempo de execução dos dois métodos com precisão e fazer a comparação.

```{python chunk7, echo=TRUE}
import time

# Executa e mede tempo de execução do método preciso
geopy_time_start = time.time()
geopy_pairwise_distances = np.empty([capitals_df.shape[0], capitals_df.shape[0]])
for i in range(capitals_df.shape[0]):
    for j in range(capitals_df.shape[0]):
        lat_i = capitals_df.iloc[i]['lat']
        lng_i = capitals_df.iloc[i]['lng']
        lat_j = capitals_df.iloc[j]['lat']
        lng_j = capitals_df.iloc[j]['lng']

        geopy_pairwise_distances[i][j] = distance.distance(
            (lat_i, lng_i),
            (lat_j, lng_j)
        ).km

geopy_time_end = time.time()
geopy_execution_time = geopy_time_end-geopy_time_start
print(f"Geopy - Tempo de execução: {np.round(geopy_execution_time, 4)} s")

# Executa e mede tempo de execução do método proposto
time_start = time.time()
capitals_df[['lat_radians', 'lng_radians']] = capitals_df[['lat', 'lng']].apply(np.radians)
capitals_df['x'] = np.cos(capitals_df['lat_radians']) * np.cos(capitals_df['lng_radians'])
capitals_df['y'] = np.cos(capitals_df['lat_radians']) * np.sin(capitals_df['lng_radians'])
capitals_df['z'] = np.sin(capitals_df['lat_radians'])
matrix = capitals_df[['x', 'y', 'z']].to_numpy()
cos = np.clip(np.dot(matrix,matrix.T), -1, 1)
theta = np.arccos(cos)

min_earth_r = 6357
max_earth_r = 6378
mid_earth_r = (min_earth_r+max_earth_r)/2
pairwise_distances = theta * mid_earth_r
time_end = time.time()
cosine_execution_time = time_end-time_start
print(f"Aproximação - Tempo de execução: {np.round(cosine_execution_time, 4)} s")

print(f"O método do cosseno é {int(geopy_execution_time/cosine_execution_time)} mais rápido.")
```

Neste exercício, estamos calculando distâncias entre 249 cidades. Em uma aplicação real, não é difícil chegar em milhares ou milhões de pontos. Neste cenário, o ganho de eficiência demonstrado aqui seria ainda mais valioso.

Em contextos práticos, um método ligeiramente menos preciso, mas substancialmente mais rápido, pode ser a chave para processar grandes volumes de dados de forma viável.

